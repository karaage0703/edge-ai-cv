{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karaage0703/edge-ai-cv/blob/main/image_classification/image_classification_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtU8eIsOsNcv"
      },
      "source": [
        "# 画像分類（PyTorch版）\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNgUsGFZBLEh"
      },
      "source": [
        "## 教師データのダウンロード\n",
        "\n",
        "ジャンケンの手の形の教師データをGitHubからダウンロード（Clone）します。\n",
        "\n",
        "2,3行目はダウンロードしたデータから、使用するデータ以外の不要なファイルを削除しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiwE2g5LB6Qn"
      },
      "source": [
        "教師データをダウンロードして、不要なファイルを削除します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JqHcRAEB2n_"
      },
      "source": [
        "!git clone https://github.com/karaage0703/janken_dataset original_datasets\n",
        "!rm -rf /content/original_datasets/.git\n",
        "!rm /content/original_datasets/LICENSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hKLAhGSCufr"
      },
      "source": [
        "データの中身の確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUU8okD-Cwuo"
      },
      "source": [
        "!ls original_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdO2sCkkGXU6"
      },
      "source": [
        "!ls original_datasets/choki"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63s1idu3Ct_Q"
      },
      "source": [
        "from IPython.display import Image as IPImage\n",
        "from IPython.display import display_jpeg\n",
        "display_jpeg(IPImage('original_datasets/choki/choki_01.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLcN1JHv2Q6g"
      },
      "source": [
        "## 教師データを訓練データ（Train Data）とテストデータ（Validation Data）に分ける"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPVitbmqrXgi"
      },
      "source": [
        "ディレクトリの構造を可視化するための'tree'というソフトをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA0Me7ARH3gJ"
      },
      "source": [
        "!apt-get -qq install tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0h2uglmIzQv"
      },
      "source": [
        "!tree -d /content/original_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zmdetFKFvWp"
      },
      "source": [
        "教師データのディレクトリと、ターゲットとなるディレクトリ（この下に訓練データのディレクトリと検証データのディレクトリが生成される）を指定。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA4SGY54GY6b"
      },
      "source": [
        "dataset_original_dir = '/content/original_datasets'\n",
        "dataset_root_dir = '/content/datasets'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGSRbbuIreaQ"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/karaage0703/karaage-ai-book/master/util/split_train_val.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMHcDDbJrvQu"
      },
      "source": [
        "import split_train_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyNShcXVsLzK"
      },
      "source": [
        "split_train_val.image_dir_train_val_split(dataset_original_dir, dataset_root_dir, train_size=0.66)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHugFL3VIJfb"
      },
      "source": [
        "!tree -d /content/datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3PHCUYZJd-W"
      },
      "source": [
        "train_dir = '/content/datasets/train'\n",
        "val_dir = '/content/datasets/val'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9mpAQuDwqnZ"
      },
      "source": [
        "### データのロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln1wU_8DFe22"
      },
      "source": [
        "必要なライブラリをインポートします\n",
        "\n",
        "PyTorchとデータの前処理や可視化をしてくれるtorchvisionという便利なライブラリをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ltMN9XGBqSa"
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_tOlCAhlg8z"
      },
      "source": [
        "ImageFolderを使って、訓練ディレクトリの画像をdataset_train、検証ディレクトリの画像を dataset_valとして読み込みます"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDht24TVlehX"
      },
      "source": [
        "dataset_train = datasets.ImageFolder(root=train_dir)\n",
        "dataset_val = datasets.ImageFolder(root=val_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4vEncBWl8lC"
      },
      "source": [
        "dataset_trainとdataset_valの中身を確認します。それぞれ 109と58のデータが格納されていることが分かります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71o0iGyknJhH"
      },
      "source": [
        "print(dataset_train)\n",
        "print(dataset_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI4VTL_NuFPm"
      },
      "source": [
        "dataset_train[0]〜dataset_train[108]の中身は、PIL形式の画像データとラベルのインデックスが格納されています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USnqTP-nt3GK"
      },
      "source": [
        "x, y = dataset_train[0]\n",
        "print(x)\n",
        "print(y)\n",
        "x, y = dataset_val[0]\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwuOg8MMtL0u"
      },
      "source": [
        "具体的な中身は`__getitem__`で確認できます。\n",
        "\n",
        "最後の数字はラベルを示しています。最初に表示させたディレクトリの表示順となり以下となります。\n",
        "\n",
        "```\n",
        "0: choki\n",
        "1: gu\n",
        "2: pa\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySJe89RMtIhm"
      },
      "source": [
        "print(dataset_train.__getitem__(0))\n",
        "print(dataset_train.__getitem__(50))\n",
        "print(dataset_train.__getitem__(100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Eq3Q_x7mWiz"
      },
      "source": [
        "matplotlibで中身を確認してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WzBNrp7mLoo"
      },
      "source": [
        "image_numb = 6 # 3の倍数を指定してください\n",
        "for i in range(0, image_numb):\n",
        "  ax = plt.subplot(int(image_numb / 2), 3, i + 1)\n",
        "  plt.tight_layout()\n",
        "  ax.set_title(str(i))\n",
        "  plt.imshow(dataset_train[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V74KU4enx2l"
      },
      "source": [
        "datasetは使えるようになりましたが、PyTorchで扱うためにサイズの変換と、テンソル化というPyTorch等のディープラーニングのフレームワークで多く使われる、Tensor形式に変換する必要があります。\n",
        "\n",
        "具体的には、transforms機能を用いて、以下の前処理を実施します。\n",
        "- リサイズ(transforms.Resize())\n",
        "- テンソル化(transforms.ToTensor())\n",
        "\n",
        "以下のように書くと、前処理を実施したデータが読み込めます\n",
        "\n",
        "参考： https://qiita.com/kazetof/items/6a72926b9f8cd44c218e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMzxuGNWnxBT"
      },
      "source": [
        "IMAGE_SIZE = 64\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset_train = datasets.ImageFolder(root=train_dir, transform=data_transform)\n",
        "dataset_val = datasets.ImageFolder(root=val_dir, transform=data_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdEEtPsWmcyW"
      },
      "source": [
        "中身を確認します。前処理情報が追記されています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3ncnHM8oaTH"
      },
      "source": [
        "print(dataset_train)\n",
        "print(dataset_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PUz8xJCukXT"
      },
      "source": [
        "生の値を確認しましょう。教師データのPIL形式の画像が、テンソル形式に変換され、値の範囲が0〜1になっていることが分かります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIrDEECaulGw"
      },
      "source": [
        "print(dataset_train.__getitem__(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzeKLb1dvPcl"
      },
      "source": [
        "また、画像のサイズを確認しておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbmhW3u1vOXJ"
      },
      "source": [
        "print(dataset_train[0][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNFMhRqTvlwU"
      },
      "source": [
        "この配列は画像のサイズを示しています。最初の3はチャンネル数です。RGB形式なので(Red, Green, Blue）の3次元となります。。\n",
        "\n",
        "そのあとの、64は先ほど示した画像のサイズを示しています。PyTorchでは最初にチャンネル数を指定することになるので、注意して下さい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm42CCRkopPL"
      },
      "source": [
        "続いて、この中身をmatplotlibで画像として確認しましょう。ただし、datasetはテンソル化されているので、datasetをnumpy()でNumpy配列とする必要があります。\n",
        "\n",
        "また、PyTorchはChannel Firstという、チャンネル（RGB画像の場合はR,G,Bの3チャンネルなので3）を一番最初に並べる形式のため、画像として表示するため、Numpyのtransposeでチャンネルを最後に並び変えます。\n",
        "\n",
        "具体的には `dataset`の後ろに `.numpy().transpose((1, 2, 0))`を追加します。\n",
        "\n",
        "ちゃんと表示されました。図の軸の数字から、サイズが64x64に変換されていることも分かります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ-vmBBVoq23"
      },
      "source": [
        "image_numb = 6 # 3の倍数を指定してください\n",
        "for i in range(0, image_numb):\n",
        "  ax = plt.subplot(int(image_numb / 2), 3, i + 1)\n",
        "  plt.tight_layout()\n",
        "  ax.set_title(str(i))\n",
        "  plt.imshow(dataset_train[i][0].numpy().transpose((1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roHDM7MB1w9S"
      },
      "source": [
        "## ラベルファイルの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSq7-H5b2DVx"
      },
      "source": [
        "学習するファイルのラベルを作成します"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN5gSnfyz7Tp"
      },
      "source": [
        "必要なライブラリをインポートします"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRQkx3zp1Pwt"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjsFU39e3ncO"
      },
      "source": [
        "データを保存する場所を指定します。\n",
        "\n",
        "今後、ラベルデータやモデルデータなどは以下のディレクトリに保存されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UhMCgO53m4M"
      },
      "source": [
        "backup_dir = './model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKkS5ed_Jx6c"
      },
      "source": [
        "ラベルデータを作成します（最後に表示される class numberが画像の種類の数です）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIhpFWwJ1Reb"
      },
      "source": [
        "labels = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
        "labels.sort()\n",
        "\n",
        "if os.path.exists(backup_dir):\n",
        "    shutil.rmtree(backup_dir)\n",
        "\n",
        "os.makedirs(backup_dir)\n",
        "\n",
        "with open(backup_dir + '/labels.txt','w') as f:\n",
        "    for label in labels:\n",
        "        f.write(label+\"\\n\")\n",
        "\n",
        "NUM_CLASSES = len(labels)\n",
        "print(\"class number=\" + str(NUM_CLASSES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzE-lgUNJ3e8"
      },
      "source": [
        "ラベルを確認します。ラベル名（choki, gu, pa）が並んでいればOKです"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbgCNdZ7LLBr"
      },
      "source": [
        "!cat ./model/labels.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PnpwxpJwjFL"
      },
      "source": [
        "## モデルの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUQ4gTmA3JVQ"
      },
      "source": [
        "ハイパーパラメータを設定します。\n",
        "\n",
        "TensorFlow（Keras）のときとは学習率が大きく異なるため注意して下さい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVSkW9C7R9Mj"
      },
      "source": [
        "# 学習率\n",
        "LEARNING_RATE = 1.0\n",
        "# エポック（世代数）\n",
        "EPOCHS = 20\n",
        "# バッチサイズ\n",
        "BATCH_SIZE = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UREAgfDzHFin"
      },
      "source": [
        "学習用のデータをデータセットからロードするデータローダーを作成します。これにより、データセットからバッチと呼ばれるデータのまとまりで、データをロードすることができます。\n",
        "\n",
        "- batch_size: バッチサイズ\n",
        "- shuffle: シャッフルするか\n",
        "- num_workers: データをロードするコア数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltll0T_DHNc8"
      },
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(dataset_train,\n",
        "                                          batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_val,\n",
        "                                          batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyf_hsSMx1R0"
      },
      "source": [
        "モデルを作成します。ポイントとなる1層目の畳み込み層のConv2dの定義は以下です。\n",
        "```\n",
        "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "```\n",
        "\n",
        "今回扱うRGB画像は、入力チャンネルがR,G,Bの3チャンネル。出力チャンネルは64, カーネルサイズは 3x3、padding=1とします。paddingを1としているのは、畳み込み前後で画像のサイズを変えないためです。\n",
        "\n",
        "具体的には以下となります。\n",
        "\n",
        "```\n",
        "nn.Conv2d(3, 32, (3, 3), 1, 1)\n",
        "```\n",
        "\n",
        "またfc層への接続は、注意が必要です。\n",
        "\n",
        "fc層(fc1) の入力は以下となります。\n",
        "\n",
        "```\n",
        "チャネル数 * イメージマップのサイズ\n",
        "```\n",
        "\n",
        "チャネル数は、conv2層の出力 64 となります。また、イメージマップのサイズは、最初 64x64だったものが、CNN層、プーリング層によりサイズが変わるため計算が必要です。\n",
        "\n",
        "CNNの場合は、カーネルサイズ、パディングによって変わりますが、今回は入力と出力でサイズが変わらないように調整してあります。\n",
        "\n",
        "マックスプーリングは、2x2で実施しているので半分になります。よって 32x32 がイメージマップのサイズとなります。\n",
        "\n",
        "全結合層へ繋げるための1次元へ展開は、x.viewを使います。-1を引数とすることで、self.num_flat_features(x)の値から自動的に値が決まります。\n",
        "\n",
        "```\n",
        "x = x.view(-1, self.num_flat_features(x))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TErR24xVayAV"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, (3, 3), 1, 1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, (3, 3), 1, 1)\n",
        "    self.dropout1 = nn.Dropout(0.25)\n",
        "    self.dropout2 = nn.Dropout(0.5)\n",
        "    self.fc1 = nn.Linear(64 * 32 * 32, 128) # 32 = 64(IMAGE_SIZE) / 2\n",
        "    self.fc2 = nn.Linear(128, NUM_CLASSES)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "    x = self.dropout1(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrJo4GmlQEby"
      },
      "source": [
        "モデルを定義して、確認します。\n",
        "\n",
        "最初の1行は、GPU(CUDA)が使えるかを判断しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O0QXFdWQEvn"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te2jjx_ZFJFx"
      },
      "source": [
        "パラメータの確認をします"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWiRX9PiFIhF"
      },
      "source": [
        "#model.parameters()をリストに型変換することでパラメータを取り出せる\n",
        "params=list(model.parameters())\n",
        "#len(params)はパラメータの種類の数\n",
        "print(len(params))\n",
        "#conv1の重みのサイズを確認する\n",
        "print(params[0].size())\n",
        "#conv2の重みのサイズを確認する\n",
        "print(params[2].size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRLlNUtXNIUy"
      },
      "source": [
        "ニューラルネットワークのテスト。ニューラルネットの入力に合わせた、教師データと同じサイズのランダムな画像をネットワークに入力して出力を確認します。\n",
        "\n",
        "`個数, チャネル数, 画像サイズ, 画像サイズ`\n",
        "\n",
        "デバイス(GPU or CPU)に応じた変換(`.to(device)`)が必要なことに注意しましょう。\n",
        "\n",
        "最終的な出力が、3つ（ラベルの数）になっていることを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PqvWENfNItW"
      },
      "source": [
        "input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "input = input.to(device)\n",
        "out = model(input)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucNcptMEQ2Ak"
      },
      "source": [
        "最適化方法を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH9pa5Z8cprk"
      },
      "source": [
        "from torch import optim\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqLJDFl3rznt"
      },
      "source": [
        "AIモデルの学習を行います"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18WChEyU3soc"
      },
      "source": [
        "def train(model, device, train_dataloader, optimizer):\n",
        "  train_loss = 0\n",
        "\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(train_dataloader)\n",
        "\n",
        "  return train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzRj2BqU80SI"
      },
      "source": [
        "def test(model, device, test_dataloader):\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  val_acc = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_dataloader):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      val_loss += F.nll_loss(output, target).item()\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      total += target.size(0)\n",
        "\n",
        "  val_loss = val_loss / len(test_dataloader)\n",
        "  val_acc = correct / total\n",
        "\n",
        "  return val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML1vEA89-4xF"
      },
      "source": [
        "train_loss_list = []\n",
        "\n",
        "val_loss_list = []\n",
        "val_acc_list = []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train_loss = train(model, device, train_dataloader, optimizer)\n",
        "  val_loss, val_acc = test(model, device, test_dataloader)\n",
        "\n",
        "  train_loss_list.append(train_loss)\n",
        "  val_loss_list.append(val_loss)\n",
        "  val_acc_list.append(val_acc)\n",
        "\n",
        "  print('epoch: {:d}'.format(epoch))\n",
        "  print('val_loss: {:.4f}, val_acc: {:.4f}'.format(100. * val_loss, val_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELuHBiwnGIk7"
      },
      "source": [
        "## 学習結果の可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZbIQD0KHgFt"
      },
      "source": [
        "lossを確認します。lossは正解との差を意味するモデルを評価するための指標で、低いほど良い値となります。\n",
        "\n",
        "AIモデルは、この値が低くなるように学習を進めます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nKjkoxfG7Ox"
      },
      "source": [
        "plt.plot(train_loss_list)\n",
        "plt.plot(val_loss_list)\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.xlim([0.0, EPOCHS])\n",
        "plt.legend(['loss', 'val_loss'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgkov1_yG5WJ"
      },
      "source": [
        "acc（精度）を確認します。accが訓練データでの精度で、この値が高いほど良い性能を意味します。\n",
        "例えば0.5だと50%の正解率ということになります。\n",
        "\n",
        "val_accというのが訓練に使っていないテストデータを使っての精度です。  \n",
        "いわゆる、本当の精度と言われるものは、val_accの方となります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj3GjxWuPOyo"
      },
      "source": [
        "plt.plot(val_acc_list)\n",
        "plt.title('Validation acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.xlim([0.0, EPOCHS])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.legend(['val_loss'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vWiax4zg70l"
      },
      "source": [
        "## 学習させたモデルを使った推定\n",
        "\n",
        "学習させたモデルを使って、画像の推定を行います"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppIMenw0c7Op"
      },
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "    fig = plt.figure(figsize=(10, num_images))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_dataloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            _, pred = torch.max(output, 1)\n",
        "\n",
        "            for n in range(data.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//3, 3, images_so_far)\n",
        "                ax.axis('off')\n",
        "                color = 'green' if pred[n] == target[n] else 'red'\n",
        "                ax.set_title('predicted: {} , label: {}'.format(labels[pred[n]], labels[target[n]]), color=color)\n",
        "                plt.imshow(data.cpu().data[n].numpy().transpose((1, 2, 0)))\n",
        "                if images_so_far == num_images:\n",
        "                    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1ZAALxuevFN"
      },
      "source": [
        "visualize_model(model, num_images=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MMDB6DdpAWY"
      },
      "source": [
        "## 混合行列(Confusion Matrix)の可視化\n",
        "\n",
        "https://stackoverflow.com/questions/53290306/confusion-matrix-and-test-accuracy-for-pytorch-transfer-learning-tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0sLFIvaet6_"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Initialize the prediction and label lists(tensors)\n",
        "predlist=torch.zeros(0, dtype=torch.long, device='cpu')\n",
        "lbllist=torch.zeros(0, dtype=torch.long, device='cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_dataloader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        # Append batch prediction results\n",
        "        predlist=torch.cat([predlist,pred.view(-1).cpu()])\n",
        "        lbllist=torch.cat([lbllist,target.view(-1).cpu()])\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
        "cm = cm/cm.sum(1)\n",
        "\n",
        "sns.heatmap(cm, annot=True, square=True, cmap=plt.cm.Blues,\n",
        "            xticklabels=labels,\n",
        "            yticklabels=labels)\n",
        "\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.xlim([0.0, NUM_CLASSES])\n",
        "plt.ylim([0.0, NUM_CLASSES])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80XaJINvnDsR"
      },
      "source": [
        "## 学習モデルの保存とダウンロード\n",
        "\n",
        "学習モデルを保存します。また、Google Colaboratory上のファイルは、自動的に消えてしまうのでモデルをローカルにダウンロードします。\n",
        "\n",
        "最初にモデルを保存します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFkpwIn6nDGM"
      },
      "source": [
        "model_path = 'janken.pth'\n",
        "torch.save(model, model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XJ6lYxjonAp"
      },
      "source": [
        "モデルをダウンロードします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_WAtb0fojhj"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TorchモデルからONNXモデルへの変換"
      ],
      "metadata": {
        "id": "ygFB2DZrYz_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_path = 'janken.onnx'\n",
        "\n",
        "# Input size(N, C, H, W)\n",
        "x = torch.randn(1, 3, 64, 64)\n",
        "x = x.to(device)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,                                         # model\n",
        "    x,                                              # input data\n",
        "    onnx_path,                                  # ONNX file name\n",
        "    opset_version=11,                               # ONNX version\n",
        "    )"
      ],
      "metadata": {
        "id": "4ufbzmzIY5eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(onnx_path)"
      ],
      "metadata": {
        "id": "xgTyEyjoZhHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3jKFzqu-00L"
      },
      "source": [
        "## まとめ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-BVvtN2aqKX"
      },
      "source": [
        "PyTorchの学習から推論とモデルの保存ができました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3H26WGdKAze"
      },
      "source": [
        "## 参考リンク"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7UIdrA1KFlO"
      },
      "source": [
        "以下は多くを参考にした情報です。\n",
        "\n",
        "前処理（データセット・データローダー）\n",
        "- https://sonaeru-blog.com/pytorch-dataset/\n",
        "- http://kaga100man.com/2019/01/09/post-89/\n",
        "- https://qiita.com/takurooo/items/e4c91c5d78059f92e76d\n",
        "- https://discuss.pytorch.org/t/questions-about-imagefolder/774/6\n",
        "\n",
        "自作データセットの学習\n",
        "- http://robonchu.hatenablog.com/entry/2017/10/23/173317\n",
        "\n",
        "ニューラルネットワーク・学習\n",
        "- https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "- http://aidiary.hatenablog.com/entry/20180205/1517832760\n",
        "- https://pytorch.org/docs/stable/nn.html\n",
        "- https://blog.shikoan.com/pytorch-convtranspose2d/\n",
        "- https://www.hellocybernetics.tech/entry/2017/10/20/025702\n",
        "- https://www.hellocybernetics.tech/entry/2018/02/20/182906\n",
        "- https://www.sambaiz.net/article/205/\n",
        "- https://www.procrasist.com/entry/19-pytorch\n",
        "- https://qiita.com/mckeeeen/items/e255b4ac1efba88d0ca1\n",
        "- https://tips-memo.com/python-pytorch-3\n",
        "- https://qiita.com/kamata1729/items/7adaead883566e3043b5\n",
        "\n",
        "認識結果の可視化\n",
        "- http://torch.classcat.com/2018/04/29/pytorch-tutorial-transfer-learning/\n",
        "\n",
        "つまづきポイント\n",
        "- https://qiita.com/takurooo/items/e356dfdeec768d8f7146\n",
        "\n",
        "モデル変換\n",
        "- https://imagingsolution.net/deep-learning/pytorch/save_pytorch_model_onnx_csharp/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh2FRf5WNd8n"
      },
      "source": [
        "## 参考書籍\n",
        "つくりながら学ぶ！PyTorchによる発展ディープラーニング"
      ]
    }
  ]
}